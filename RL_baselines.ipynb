{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method SubprocVecEnv.__del__ of <baselines.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7fd34bf38828>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/alphalearn0/baselines/baselines/common/vec_env/subproc_vec_env.py\", line 121, in __del__\n",
      "    self.close()\n",
      "  File \"/home/alphalearn0/baselines/baselines/common/vec_env/vec_env.py\", line 98, in close\n",
      "    self.close_extras()\n",
      "  File \"/home/alphalearn0/baselines/baselines/common/vec_env/subproc_vec_env.py\", line 104, in close_extras\n",
      "    remote.send(('close', None))\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 206, in send\n",
      "    self._send_bytes(_ForkingPickler.dumps(obj))\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done! total reward: time=1993, reward=0\n",
      "press enter to continue\n",
      "\n",
      "t=1194 p=0 got penalty: -1, current reward: -1\n",
      "t=1195 p=0 got penalty: -1, current reward: -2\n",
      "t=1196 p=0 got penalty: -1, current reward: -3\n",
      "t=1197 p=0 got penalty: -1, current reward: -4\n",
      "t=1198 p=0 got penalty: -1, current reward: -5\n",
      "t=1199 p=0 got penalty: -1, current reward: -6\n",
      "t=1200 p=0 got penalty: -1, current reward: -7\n",
      "t=1201 p=0 got penalty: -1, current reward: -8\n",
      "t=1202 p=0 got penalty: -1, current reward: -9\n",
      "t=1203 p=0 got penalty: -1, current reward: -10\n",
      "t=1204 p=0 got penalty: -1, current reward: -11\n",
      "t=1205 p=0 got penalty: -1, current reward: -12\n",
      "t=1206 p=0 got penalty: -1, current reward: -13\n",
      "t=1207 p=0 got penalty: -1, current reward: -14\n",
      "t=1208 p=0 got penalty: -1, current reward: -15\n",
      "t=1209 p=0 got penalty: -1, current reward: -16\n",
      "t=1210 p=0 got penalty: -1, current reward: -17\n",
      "t=1211 p=0 got penalty: -1, current reward: -18\n",
      "t=1212 p=0 got penalty: -1, current reward: -19\n",
      "t=1213 p=0 got penalty: -1, current reward: -20\n",
      "t=1214 p=0 got penalty: -1, current reward: -21\n",
      "t=1215 p=0 got penalty: -1, current reward: -22\n",
      "t=1216 p=0 got penalty: -1, current reward: -23\n",
      "t=1217 p=0 got penalty: -1, current reward: -24\n",
      "t=1218 p=0 got penalty: -1, current reward: -25\n",
      "t=1219 p=0 got penalty: -1, current reward: -26\n",
      "t=1220 p=0 got penalty: -1, current reward: -27\n",
      "t=1221 p=0 got penalty: -1, current reward: -28\n",
      "t=1222 p=0 got penalty: -1, current reward: -29\n",
      "t=1223 p=0 got penalty: -1, current reward: -30\n",
      "t=1224 p=0 got penalty: -1, current reward: -31\n",
      "t=1225 p=0 got penalty: -1, current reward: -32\n",
      "t=1226 p=0 got penalty: -1, current reward: -33\n",
      "t=1227 p=0 got penalty: -1, current reward: -34\n",
      "t=1228 p=0 got penalty: -1, current reward: -35\n",
      "t=1229 p=0 got penalty: -1, current reward: -36\n",
      "t=1230 p=0 got penalty: -1, current reward: -37\n",
      "t=1231 p=0 got penalty: -1, current reward: -38\n",
      "t=1232 p=0 got penalty: -1, current reward: -39\n",
      "t=1233 p=0 got penalty: -1, current reward: -40\n",
      "t=1234 p=0 got penalty: -1, current reward: -41\n",
      "t=1235 p=0 got penalty: -1, current reward: -42\n",
      "t=1236 p=0 got penalty: -1, current reward: -43\n",
      "t=1237 p=0 got penalty: -1, current reward: -44\n",
      "t=1238 p=0 got penalty: -1, current reward: -45\n",
      "t=1239 p=0 got penalty: -1, current reward: -46\n",
      "t=1240 p=0 got penalty: -1, current reward: -47\n",
      "t=1241 p=0 got penalty: -1, current reward: -48\n",
      "t=1242 p=0 got penalty: -1, current reward: -49\n",
      "t=1243 p=0 got penalty: -1, current reward: -50\n",
      "t=1244 p=0 got penalty: -1, current reward: -51\n",
      "t=1245 p=0 got penalty: -1, current reward: -52\n",
      "t=1246 p=0 got penalty: -1, current reward: -53\n",
      "t=1247 p=0 got penalty: -1, current reward: -54\n",
      "t=1248 p=0 got penalty: -1, current reward: -55\n",
      "t=1249 p=0 got penalty: -1, current reward: -56\n",
      "t=1250 p=0 got penalty: -1, current reward: -57\n",
      "t=1251 p=0 got penalty: -1, current reward: -58\n",
      "t=1252 p=0 got penalty: -1, current reward: -59\n",
      "t=1253 p=0 got penalty: -1, current reward: -60\n",
      "t=1254 p=0 got penalty: -1, current reward: -61\n",
      "t=1255 p=0 got penalty: -1, current reward: -62\n",
      "t=1256 p=0 got penalty: -1, current reward: -63\n",
      "t=1257 p=0 got penalty: -1, current reward: -64\n",
      "t=1258 p=0 got penalty: -1, current reward: -65\n",
      "t=1259 p=0 got penalty: -1, current reward: -66\n",
      "t=1260 p=0 got penalty: -1, current reward: -67\n",
      "t=1261 p=0 got penalty: -1, current reward: -68\n",
      "t=1262 p=0 got penalty: -1, current reward: -69\n",
      "t=1263 p=0 got penalty: -1, current reward: -70\n",
      "t=1264 p=0 got penalty: -1, current reward: -71\n",
      "t=1265 p=0 got penalty: -1, current reward: -72\n",
      "t=1266 p=0 got penalty: -1, current reward: -73\n",
      "t=1267 p=0 got penalty: -1, current reward: -74\n",
      "t=1268 p=0 got penalty: -1, current reward: -75\n",
      "t=1269 p=0 got penalty: -1, current reward: -76\n",
      "t=1270 p=0 got penalty: -1, current reward: -77\n",
      "t=1271 p=0 got penalty: -1, current reward: -78\n",
      "t=1272 p=0 got penalty: -1, current reward: -79\n",
      "t=1273 p=0 got penalty: -1, current reward: -80\n",
      "t=1274 p=0 got penalty: -1, current reward: -81\n",
      "t=1275 p=0 got penalty: -1, current reward: -82\n",
      "t=1276 p=0 got penalty: -1, current reward: -83\n",
      "t=1277 p=0 got penalty: -1, current reward: -84\n",
      "t=1278 p=0 got penalty: -1, current reward: -85\n",
      "t=1279 p=0 got penalty: -1, current reward: -86\n",
      "t=1280 p=0 got penalty: -1, current reward: -87\n",
      "t=1281 p=0 got penalty: -1, current reward: -88\n",
      "t=1282 p=0 got penalty: -1, current reward: -89\n",
      "t=1283 p=0 got penalty: -1, current reward: -90\n",
      "t=1284 p=0 got penalty: -1, current reward: -91\n",
      "t=1285 p=0 got penalty: -1, current reward: -92\n",
      "t=1286 p=0 got penalty: -1, current reward: -93\n",
      "t=1287 p=0 got penalty: -1, current reward: -94\n",
      "t=1288 p=0 got penalty: -1, current reward: -95\n",
      "t=1289 p=0 got penalty: -1, current reward: -96\n",
      "t=1290 p=0 got penalty: -1, current reward: -97\n",
      "t=1291 p=0 got penalty: -1, current reward: -98\n",
      "t=1292 p=0 got penalty: -1, current reward: -99\n",
      "t=1293 p=0 got penalty: -1, current reward: -100\n",
      "t=1294 p=0 got penalty: -1, current reward: -101\n",
      "t=1295 p=0 got penalty: -1, current reward: -102\n",
      "t=1296 p=0 got penalty: -1, current reward: -103\n",
      "t=1297 p=0 got penalty: -1, current reward: -104\n",
      "t=1298 p=0 got penalty: -1, current reward: -105\n",
      "t=1299 p=0 got penalty: -1, current reward: -106\n",
      "t=1300 p=0 got penalty: -1, current reward: -107\n",
      "t=1301 p=0 got penalty: -1, current reward: -108\n",
      "t=1302 p=0 got penalty: -1, current reward: -109\n",
      "t=1303 p=0 got penalty: -1, current reward: -110\n",
      "t=1304 p=0 got penalty: -1, current reward: -111\n",
      "t=1305 p=0 got penalty: -1, current reward: -112\n",
      "t=1306 p=0 got penalty: -1, current reward: -113\n",
      "t=1307 p=0 got penalty: -1, current reward: -114\n",
      "t=1308 p=0 got penalty: -1, current reward: -115\n",
      "t=1309 p=0 got penalty: -1, current reward: -116\n",
      "t=1310 p=0 got penalty: -1, current reward: -117\n",
      "t=1311 p=0 got penalty: -1, current reward: -118\n",
      "t=1312 p=0 got penalty: -1, current reward: -119\n",
      "t=1313 p=0 got penalty: -1, current reward: -120\n",
      "t=1314 p=0 got penalty: -1, current reward: -121\n",
      "t=1315 p=0 got penalty: -1, current reward: -122\n",
      "t=1316 p=0 got penalty: -1, current reward: -123\n",
      "t=1317 p=0 got penalty: -1, current reward: -124\n",
      "t=1318 p=0 got penalty: -1, current reward: -125\n",
      "t=1319 p=0 got penalty: -1, current reward: -126\n",
      "t=1320 p=0 got penalty: -1, current reward: -127\n",
      "t=1321 p=0 got penalty: -1, current reward: -128\n",
      "t=1322 p=0 got penalty: -1, current reward: -129\n",
      "t=1323 p=0 got penalty: -1, current reward: -130\n",
      "t=1324 p=0 got penalty: -1, current reward: -131\n",
      "t=1325 p=0 got penalty: -1, current reward: -132\n",
      "t=1326 p=0 got penalty: -1, current reward: -133\n",
      "t=1327 p=0 got penalty: -1, current reward: -134\n",
      "t=1328 p=0 got penalty: -1, current reward: -135\n",
      "t=1329 p=0 got penalty: -1, current reward: -136\n",
      "t=1330 p=0 got penalty: -1, current reward: -137\n",
      "t=1331 p=0 got penalty: -1, current reward: -138\n",
      "t=1332 p=0 got penalty: -1, current reward: -139\n",
      "t=1333 p=0 got penalty: -1, current reward: -140\n",
      "t=1334 p=0 got penalty: -1, current reward: -141\n",
      "t=1335 p=0 got penalty: -1, current reward: -142\n",
      "t=1336 p=0 got penalty: -1, current reward: -143\n",
      "t=1337 p=0 got penalty: -1, current reward: -144\n",
      "t=1338 p=0 got penalty: -1, current reward: -145\n",
      "t=1339 p=0 got penalty: -1, current reward: -146\n",
      "t=1340 p=0 got penalty: -1, current reward: -147\n",
      "t=1341 p=0 got penalty: -1, current reward: -148\n",
      "t=1342 p=0 got penalty: -1, current reward: -149\n",
      "t=1343 p=0 got penalty: -1, current reward: -150\n",
      "t=1344 p=0 got penalty: -1, current reward: -151\n",
      "t=1345 p=0 got penalty: -1, current reward: -152\n",
      "t=1346 p=0 got penalty: -1, current reward: -153\n",
      "t=1347 p=0 got penalty: -1, current reward: -154\n",
      "t=1348 p=0 got penalty: -1, current reward: -155\n",
      "t=1349 p=0 got penalty: -1, current reward: -156\n",
      "t=1350 p=0 got penalty: -1, current reward: -157\n",
      "t=1351 p=0 got penalty: -1, current reward: -158\n",
      "t=1352 p=0 got penalty: -1, current reward: -159\n",
      "t=1353 p=0 got penalty: -1, current reward: -160\n",
      "t=1354 p=0 got penalty: -1, current reward: -161\n",
      "t=1355 p=0 got penalty: -1, current reward: -162\n",
      "t=1356 p=0 got penalty: -1, current reward: -163\n",
      "t=1357 p=0 got penalty: -1, current reward: -164\n",
      "t=1358 p=0 got penalty: -1, current reward: -165\n",
      "t=1359 p=0 got penalty: -1, current reward: -166\n",
      "t=1360 p=0 got penalty: -1, current reward: -167\n",
      "t=1361 p=0 got penalty: -1, current reward: -168\n",
      "t=1362 p=0 got penalty: -1, current reward: -169\n",
      "t=1363 p=0 got penalty: -1, current reward: -170\n",
      "t=1364 p=0 got penalty: -1, current reward: -171\n",
      "t=1365 p=0 got penalty: -1, current reward: -172\n",
      "t=1366 p=0 got penalty: -1, current reward: -173\n",
      "t=1367 p=0 got penalty: -1, current reward: -174\n",
      "t=1368 p=0 got penalty: -1, current reward: -175\n",
      "t=1369 p=0 got penalty: -1, current reward: -176\n",
      "t=1370 p=0 got penalty: -1, current reward: -177\n",
      "t=1371 p=0 got penalty: -1, current reward: -178\n",
      "t=1372 p=0 got penalty: -1, current reward: -179\n",
      "t=1373 p=0 got penalty: -1, current reward: -180\n",
      "t=1374 p=0 got penalty: -1, current reward: -181\n",
      "t=1375 p=0 got penalty: -1, current reward: -182\n",
      "t=1376 p=0 got penalty: -1, current reward: -183\n",
      "t=1377 p=0 got penalty: -1, current reward: -184\n",
      "t=1378 p=0 got penalty: -1, current reward: -185\n",
      "t=1379 p=0 got penalty: -1, current reward: -186\n",
      "t=1380 p=0 got penalty: -1, current reward: -187\n",
      "t=1381 p=0 got penalty: -1, current reward: -188\n",
      "t=1382 p=0 got penalty: -1, current reward: -189\n",
      "t=1383 p=0 got penalty: -1, current reward: -190\n",
      "t=1384 p=0 got penalty: -1, current reward: -191\n",
      "t=1385 p=0 got penalty: -1, current reward: -192\n",
      "t=1386 p=0 got penalty: -1, current reward: -193\n",
      "t=1387 p=0 got penalty: -1, current reward: -194\n",
      "t=1388 p=0 got penalty: -1, current reward: -195\n",
      "t=1389 p=0 got penalty: -1, current reward: -196\n",
      "t=1390 p=0 got penalty: -1, current reward: -197\n",
      "t=1391 p=0 got penalty: -1, current reward: -198\n",
      "t=1392 p=0 got penalty: -1, current reward: -199\n",
      "t=1393 p=0 got penalty: -1, current reward: -200\n",
      "t=1394 p=0 got penalty: -1, current reward: -201\n",
      "t=1395 p=0 got penalty: -1, current reward: -202\n",
      "t=1396 p=0 got penalty: -1, current reward: -203\n",
      "t=1397 p=0 got penalty: -1, current reward: -204\n",
      "t=1398 p=0 got penalty: -1, current reward: -205\n",
      "t=1399 p=0 got penalty: -1, current reward: -206\n",
      "t=1400 p=0 got penalty: -1, current reward: -207\n",
      "t=1401 p=0 got penalty: -1, current reward: -208\n",
      "t=1402 p=0 got penalty: -1, current reward: -209\n",
      "t=1403 p=0 got penalty: -1, current reward: -210\n",
      "t=1404 p=0 got penalty: -1, current reward: -211\n",
      "t=1405 p=0 got penalty: -1, current reward: -212\n",
      "t=1406 p=0 got penalty: -1, current reward: -213\n",
      "t=1407 p=0 got penalty: -1, current reward: -214\n",
      "t=1408 p=0 got penalty: -1, current reward: -215\n",
      "t=1409 p=0 got penalty: -1, current reward: -216\n",
      "t=1410 p=0 got penalty: -1, current reward: -217\n",
      "t=1411 p=0 got penalty: -1, current reward: -218\n",
      "t=1412 p=0 got penalty: -1, current reward: -219\n",
      "t=1413 p=0 got penalty: -1, current reward: -220\n",
      "t=1414 p=0 got penalty: -1, current reward: -221\n",
      "t=1415 p=0 got penalty: -1, current reward: -222\n",
      "t=1416 p=0 got penalty: -1, current reward: -223\n",
      "t=1417 p=0 got penalty: -1, current reward: -224\n",
      "t=1418 p=0 got penalty: -1, current reward: -225\n",
      "t=1419 p=0 got penalty: -1, current reward: -226\n",
      "t=1420 p=0 got penalty: -1, current reward: -227\n",
      "t=1421 p=0 got penalty: -1, current reward: -228\n",
      "t=1422 p=0 got penalty: -1, current reward: -229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=1423 p=0 got penalty: -1, current reward: -230\n",
      "t=1424 p=0 got penalty: -1, current reward: -231\n",
      "t=1425 p=0 got penalty: -1, current reward: -232\n",
      "t=1426 p=0 got penalty: -1, current reward: -233\n",
      "t=1427 p=0 got penalty: -1, current reward: -234\n",
      "t=1428 p=0 got penalty: -1, current reward: -235\n",
      "t=1429 p=0 got penalty: -1, current reward: -236\n",
      "t=1430 p=0 got penalty: -1, current reward: -237\n",
      "t=1431 p=0 got penalty: -1, current reward: -238\n",
      "t=1432 p=0 got penalty: -1, current reward: -239\n",
      "t=1433 p=0 got penalty: -1, current reward: -240\n",
      "t=1434 p=0 got penalty: -1, current reward: -241\n",
      "t=1435 p=0 got penalty: -1, current reward: -242\n",
      "t=1436 p=0 got penalty: -1, current reward: -243\n",
      "t=1437 p=0 got penalty: -1, current reward: -244\n",
      "t=1438 p=0 got penalty: -1, current reward: -245\n",
      "t=1439 p=0 got penalty: -1, current reward: -246\n",
      "t=1440 p=0 got penalty: -1, current reward: -247\n",
      "t=1441 p=0 got penalty: -1, current reward: -248\n",
      "t=1442 p=0 got penalty: -1, current reward: -249\n",
      "t=1443 p=0 got penalty: -1, current reward: -250\n",
      "t=1444 p=0 got penalty: -1, current reward: -251\n",
      "t=1445 p=0 got penalty: -1, current reward: -252\n",
      "t=1446 p=0 got penalty: -1, current reward: -253\n",
      "t=1447 p=0 got penalty: -1, current reward: -254\n",
      "t=1448 p=0 got penalty: -1, current reward: -255\n",
      "t=1449 p=0 got penalty: -1, current reward: -256\n",
      "t=1450 p=0 got penalty: -1, current reward: -257\n",
      "t=1451 p=0 got penalty: -1, current reward: -258\n",
      "t=1452 p=0 got penalty: -1, current reward: -259\n",
      "t=1453 p=0 got penalty: -1, current reward: -260\n",
      "t=1454 p=0 got penalty: -1, current reward: -261\n",
      "t=1455 p=0 got penalty: -1, current reward: -262\n",
      "t=1456 p=0 got penalty: -1, current reward: -263\n",
      "t=1457 p=0 got penalty: -1, current reward: -264\n",
      "t=1458 p=0 got penalty: -1, current reward: -265\n",
      "t=1459 p=0 got penalty: -1, current reward: -266\n",
      "t=1460 p=0 got penalty: -1, current reward: -267\n",
      "t=1461 p=0 got penalty: -1, current reward: -268\n",
      "t=1462 p=0 got penalty: -1, current reward: -269\n",
      "t=1463 p=0 got penalty: -1, current reward: -270\n",
      "t=1464 p=0 got penalty: -1, current reward: -271\n",
      "t=1465 p=0 got penalty: -1, current reward: -272\n",
      "t=1466 p=0 got penalty: -1, current reward: -273\n",
      "t=1467 p=0 got penalty: -1, current reward: -274\n",
      "t=1468 p=0 got penalty: -1, current reward: -275\n",
      "t=1469 p=0 got penalty: -1, current reward: -276\n",
      "t=1612 p=0 got penalty: -1, current reward: -277\n",
      "t=1613 p=0 got penalty: -1, current reward: -278\n",
      "t=1614 p=0 got penalty: -1, current reward: -279\n",
      "t=1615 p=0 got penalty: -1, current reward: -280\n",
      "t=1616 p=0 got penalty: -1, current reward: -281\n",
      "t=1617 p=0 got penalty: -1, current reward: -282\n",
      "t=1618 p=0 got penalty: -1, current reward: -283\n",
      "t=1619 p=0 got penalty: -1, current reward: -284\n",
      "t=1620 p=0 got penalty: -1, current reward: -285\n",
      "t=1621 p=0 got penalty: -1, current reward: -286\n",
      "t=1622 p=0 got penalty: -1, current reward: -287\n",
      "t=1623 p=0 got penalty: -1, current reward: -288\n",
      "t=1624 p=0 got penalty: -1, current reward: -289\n",
      "t=1625 p=0 got penalty: -1, current reward: -290\n",
      "t=1626 p=0 got penalty: -1, current reward: -291\n",
      "t=1627 p=0 got penalty: -1, current reward: -292\n",
      "t=1628 p=0 got penalty: -1, current reward: -293\n",
      "t=1629 p=0 got penalty: -1, current reward: -294\n",
      "t=1630 p=0 got penalty: -1, current reward: -295\n",
      "t=1631 p=0 got penalty: -1, current reward: -296\n",
      "t=1632 p=0 got penalty: -1, current reward: -297\n",
      "t=1633 p=0 got penalty: -1, current reward: -298\n",
      "t=1634 p=0 got penalty: -1, current reward: -299\n",
      "t=1635 p=0 got penalty: -1, current reward: -300\n",
      "t=1636 p=0 got penalty: -1, current reward: -301\n",
      "t=1637 p=0 got penalty: -1, current reward: -302\n",
      "t=1638 p=0 got penalty: -1, current reward: -303\n",
      "t=1639 p=0 got penalty: -1, current reward: -304\n",
      "t=1640 p=0 got penalty: -1, current reward: -305\n",
      "t=1641 p=0 got penalty: -1, current reward: -306\n",
      "t=1642 p=0 got penalty: -1, current reward: -307\n",
      "t=1643 p=0 got penalty: -1, current reward: -308\n",
      "t=1644 p=0 got penalty: -1, current reward: -309\n",
      "t=1645 p=0 got penalty: -1, current reward: -310\n",
      "t=1646 p=0 got penalty: -1, current reward: -311\n",
      "t=1647 p=0 got penalty: -1, current reward: -312\n",
      "t=1648 p=0 got penalty: -1, current reward: -313\n",
      "t=1649 p=0 got penalty: -1, current reward: -314\n",
      "t=1650 p=0 got penalty: -1, current reward: -315\n",
      "t=1651 p=0 got penalty: -1, current reward: -316\n",
      "t=1652 p=0 got penalty: -1, current reward: -317\n",
      "t=1653 p=0 got penalty: -1, current reward: -318\n",
      "t=1654 p=0 got penalty: -1, current reward: -319\n",
      "t=1655 p=0 got penalty: -1, current reward: -320\n",
      "t=1656 p=0 got penalty: -1, current reward: -321\n",
      "t=1657 p=0 got penalty: -1, current reward: -322\n",
      "t=1658 p=0 got penalty: -1, current reward: -323\n",
      "t=1659 p=0 got penalty: -1, current reward: -324\n",
      "t=1660 p=0 got penalty: -1, current reward: -325\n",
      "t=1661 p=0 got penalty: -1, current reward: -326\n",
      "t=1662 p=0 got penalty: -1, current reward: -327\n",
      "t=1663 p=0 got penalty: -1, current reward: -328\n",
      "t=1664 p=0 got penalty: -1, current reward: -329\n",
      "t=1665 p=0 got penalty: -1, current reward: -330\n",
      "t=1666 p=0 got penalty: -1, current reward: -331\n",
      "t=1667 p=0 got penalty: -1, current reward: -332\n",
      "t=1668 p=0 got penalty: -1, current reward: -333\n",
      "t=1669 p=0 got penalty: -1, current reward: -334\n",
      "t=1670 p=0 got penalty: -1, current reward: -335\n",
      "t=1671 p=0 got penalty: -1, current reward: -336\n",
      "t=1672 p=0 got penalty: -1, current reward: -337\n",
      "t=1673 p=0 got penalty: -1, current reward: -338\n",
      "t=1674 p=0 got penalty: -1, current reward: -339\n",
      "t=1675 p=0 got penalty: -1, current reward: -340\n",
      "t=1676 p=0 got penalty: -1, current reward: -341\n",
      "t=1677 p=0 got penalty: -1, current reward: -342\n",
      "t=1678 p=0 got penalty: -1, current reward: -343\n",
      "t=1679 p=0 got penalty: -1, current reward: -344\n",
      "t=1680 p=0 got penalty: -1, current reward: -345\n",
      "t=1681 p=0 got penalty: -1, current reward: -346\n",
      "t=1682 p=0 got penalty: -1, current reward: -347\n",
      "t=1683 p=0 got penalty: -1, current reward: -348\n",
      "t=1684 p=0 got penalty: -1, current reward: -349\n",
      "t=1685 p=0 got penalty: -1, current reward: -350\n",
      "t=1686 p=0 got penalty: -1, current reward: -351\n",
      "t=1687 p=0 got penalty: -1, current reward: -352\n",
      "t=1688 p=0 got penalty: -1, current reward: -353\n",
      "t=1689 p=0 got penalty: -1, current reward: -354\n",
      "t=1690 p=0 got penalty: -1, current reward: -355\n",
      "t=1691 p=0 got penalty: -1, current reward: -356\n",
      "t=1692 p=0 got penalty: -1, current reward: -357\n",
      "t=1693 p=0 got penalty: -1, current reward: -358\n",
      "t=1694 p=0 got penalty: -1, current reward: -359\n",
      "t=1695 p=0 got penalty: -1, current reward: -360\n",
      "t=1696 p=0 got penalty: -1, current reward: -361\n",
      "t=1697 p=0 got penalty: -1, current reward: -362\n",
      "t=1698 p=0 got penalty: -1, current reward: -363\n",
      "t=1699 p=0 got penalty: -1, current reward: -364\n",
      "t=1700 p=0 got penalty: -1, current reward: -365\n",
      "t=1701 p=0 got penalty: -1, current reward: -366\n",
      "t=1702 p=0 got penalty: -1, current reward: -367\n",
      "t=1703 p=0 got penalty: -1, current reward: -368\n",
      "t=1704 p=0 got penalty: -1, current reward: -369\n",
      "t=1705 p=0 got penalty: -1, current reward: -370\n",
      "t=1706 p=0 got penalty: -1, current reward: -371\n",
      "t=1707 p=0 got penalty: -1, current reward: -372\n",
      "t=1708 p=0 got penalty: -1, current reward: -373\n",
      "t=1709 p=0 got penalty: -1, current reward: -374\n",
      "t=1710 p=0 got penalty: -1, current reward: -375\n",
      "t=1711 p=0 got penalty: -1, current reward: -376\n",
      "t=1712 p=0 got penalty: -1, current reward: -377\n",
      "t=1713 p=0 got penalty: -1, current reward: -378\n",
      "t=1714 p=0 got penalty: -1, current reward: -379\n",
      "t=1715 p=0 got penalty: -1, current reward: -380\n",
      "t=1716 p=0 got penalty: -1, current reward: -381\n",
      "t=1717 p=0 got penalty: -1, current reward: -382\n",
      "t=1718 p=0 got penalty: -1, current reward: -383\n",
      "t=1719 p=0 got penalty: -1, current reward: -384\n",
      "t=1720 p=0 got penalty: -1, current reward: -385\n",
      "t=1721 p=0 got penalty: -1, current reward: -386\n",
      "t=1722 p=0 got penalty: -1, current reward: -387\n",
      "t=1723 p=0 got penalty: -1, current reward: -388\n",
      "t=1724 p=0 got penalty: -1, current reward: -389\n",
      "t=1725 p=0 got penalty: -1, current reward: -390\n",
      "t=1726 p=0 got penalty: -1, current reward: -391\n",
      "t=1727 p=0 got penalty: -1, current reward: -392\n",
      "t=1728 p=0 got penalty: -1, current reward: -393\n",
      "t=1729 p=0 got penalty: -1, current reward: -394\n",
      "t=1730 p=0 got penalty: -1, current reward: -395\n",
      "t=1731 p=0 got penalty: -1, current reward: -396\n",
      "t=1732 p=0 got penalty: -1, current reward: -397\n",
      "t=1733 p=0 got penalty: -1, current reward: -398\n",
      "t=1734 p=0 got penalty: -1, current reward: -399\n",
      "t=1735 p=0 got penalty: -1, current reward: -400\n",
      "t=1736 p=0 got penalty: -1, current reward: -401\n",
      "t=1737 p=0 got penalty: -1, current reward: -402\n",
      "t=1738 p=0 got penalty: -1, current reward: -403\n",
      "t=1739 p=0 got penalty: -1, current reward: -404\n",
      "t=1740 p=0 got penalty: -1, current reward: -405\n",
      "t=1741 p=0 got penalty: -1, current reward: -406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=2644 p=0 got penalty: -100, current reward: -506\n",
      "t=8793 p=0 got penalty: -100, current reward: -606\n",
      "t=13033 p=0 got penalty: -100, current reward: -706\n"
     ]
    }
   ],
   "source": [
    "import retro\n",
    "\n",
    "game = 'Pitfall-Atari2600'\n",
    "scenario = 'scenario'\n",
    "record = False\n",
    "verbose = 1\n",
    "quiet = 0\n",
    "obs_type = 'image'\n",
    "players = 1\n",
    "\n",
    "'''\n",
    "parser.add_argument('--game', default=GAME, help='the name or path for the game to run')\n",
    "parser.add_argument('--state', help='the initial state file to load, minus thes extension')\n",
    "parser.add_argument('--scenario', '-s', default='scenario', help='the scenario file to load, minus the extension')\n",
    "parser.add_argument('--record', '-r', action='store_true', help='record bk2 movies')\n",
    "parser.add_argument('--verbose', '-v', action='count', default=1, help='increase verbosity (can be specified multiple times)')\n",
    "parser.add_argument('--quiet', '-q', action='count', default=0, help='decrease verbosity (can be specified multiple times)')\n",
    "parser.add_argument('--obs-type', '-o', default='image', choices=['image', 'ram'], help='the observation type, either `image` (default) or `ram`')\n",
    "parser.add_argument('--players', '-p', type=int, default=1, help='number of players/agents (default: 1)')\n",
    "args = parser.parse_args()\n",
    "'''\n",
    "\n",
    "obs_type = retro.Observations.IMAGE if obs_type == 'image' else retro.Observations.RAM\n",
    "env = retro.make(game, retro.State.DEFAULT, scenario=scenario, record=record, players=players, obs_type=obs_type)\n",
    "verbosity = verbose - quiet\n",
    "try:\n",
    "    while True:\n",
    "        ob = env.reset()\n",
    "        t = 0\n",
    "        totrew = [0] * players\n",
    "        while True:\n",
    "            ac = env.action_space.sample()\n",
    "            ob, rew, done, info = env.step(ac)\n",
    "            t += 1\n",
    "            if t % 10 == 0:\n",
    "                if verbosity > 1:\n",
    "                    infostr = ''\n",
    "                    if info:\n",
    "                        infostr = ', info: ' + ', '.join(['%s=%i' % (k, v) for k, v in info.items()])\n",
    "                    print(('t=%i' % t) + infostr)\n",
    "                env.render()\n",
    "            if players == 1:\n",
    "                rew = [rew]\n",
    "            for i, r in enumerate(rew):\n",
    "                totrew[i] += r\n",
    "                if verbosity > 0:\n",
    "                    if r > 0:\n",
    "                        print('t=%i p=%i got reward: %g, current reward: %g' % (t, i, r, totrew[i]))\n",
    "                    if r < 0:\n",
    "                        print('t=%i p=%i got penalty: %g, current reward: %g' % (t, i, r, totrew[i]))\n",
    "            if done:\n",
    "                env.render()\n",
    "                try:\n",
    "                    if verbosity >= 0:\n",
    "                        if players > 1:\n",
    "                            print(\"done! total reward: time=%i, reward=%r\" % (t, totrew))\n",
    "                        else:\n",
    "                            print(\"done! total reward: time=%i, reward=%d\" % (t, totrew[0]))\n",
    "                        input(\"press enter to continue\")\n",
    "                        print()\n",
    "                    else:\n",
    "                        input(\"\")\n",
    "                except EOFError:\n",
    "                    exit(0)\n",
    "                break\n",
    "except KeyboardInterrupt:\n",
    "    exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alphalearn0/openai/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/alphalearn0/openai/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/alphalearn0/openai/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/alphalearn0/openai/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/alphalearn0/openai/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/alphalearn0/openai/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/alphalearn0/openai/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/alphalearn0/openai/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/alphalearn0/openai/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/alphalearn0/openai/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/alphalearn0/openai/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/alphalearn0/openai/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/alphalearn0/baselines/baselines/common/misc_util.py:58: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/alphalearn0/baselines/baselines/common/tf_util.py:53: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/alphalearn0/baselines/baselines/common/tf_util.py:63: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/alphalearn0/baselines/baselines/common/tf_util.py:70: The name tf.InteractiveSession is deprecated. Please use tf.compat.v1.InteractiveSession instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/alphalearn0/baselines/baselines/ppo2/model.py:34: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/alphalearn0/baselines/baselines/common/input.py:57: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /home/alphalearn0/baselines/baselines/common/policies.py:43: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f095cdc6630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f095cdc6630>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f095cdc6630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f095cdc6630>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f095c92c828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f095c92c828>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f095c92c828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f095c92c828>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:From /home/alphalearn0/openai/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f095c890da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f095c890da0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f095c890da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f095c890da0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f095c890da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f095c890da0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f095c890da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f095c890da0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:From /home/alphalearn0/baselines/baselines/ppo2/model.py:100: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "Logging to /tmp/openai-2019-12-10-06-49-50-148438\n",
      "Stepping environment...\n",
      "Done.\n",
      "---------------------------------------\n",
      "| eplenmean               | nan       |\n",
      "| eprewmean               | nan       |\n",
      "| fps                     | 416       |\n",
      "| loss/approxkl           | 2.88e-06  |\n",
      "| loss/clipfrac           | 0         |\n",
      "| loss/policy_entropy     | 5.55      |\n",
      "| loss/policy_loss        | -0.000161 |\n",
      "| loss/value_loss         | 1.86e-07  |\n",
      "| misc/explained_variance | -0.0359   |\n",
      "| misc/nupdates           | 1         |\n",
      "| misc/serial_timesteps   | 128       |\n",
      "| misc/time_elapsed       | 2.46      |\n",
      "| misc/total_timesteps    | 1.02e+03  |\n",
      "---------------------------------------\n",
      "Stepping environment...\n",
      "Done.\n",
      "---------------------------------------\n",
      "| eplenmean               | nan       |\n",
      "| eprewmean               | nan       |\n",
      "| fps                     | 873       |\n",
      "| loss/approxkl           | 6.5e-06   |\n",
      "| loss/clipfrac           | 0         |\n",
      "| loss/policy_entropy     | 5.55      |\n",
      "| loss/policy_loss        | -5.87e-05 |\n",
      "| loss/value_loss         | 1.74e-07  |\n",
      "| misc/explained_variance | 0.342     |\n",
      "| misc/nupdates           | 2         |\n",
      "| misc/serial_timesteps   | 256       |\n",
      "| misc/time_elapsed       | 3.64      |\n",
      "| misc/total_timesteps    | 2.05e+03  |\n",
      "---------------------------------------\n",
      "Stepping environment...\n",
      "Done.\n",
      "---------------------------------------\n",
      "| eplenmean               | nan       |\n",
      "| eprewmean               | nan       |\n",
      "| fps                     | 870       |\n",
      "| loss/approxkl           | 1.4e-05   |\n",
      "| loss/clipfrac           | 0         |\n",
      "| loss/policy_entropy     | 5.55      |\n",
      "| loss/policy_loss        | -0.000181 |\n",
      "| loss/value_loss         | 6.63e-08  |\n",
      "| misc/explained_variance | 0.062     |\n",
      "| misc/nupdates           | 3         |\n",
      "| misc/serial_timesteps   | 384       |\n",
      "| misc/time_elapsed       | 4.81      |\n",
      "| misc/total_timesteps    | 3.07e+03  |\n",
      "---------------------------------------\n",
      "Stepping environment...\n",
      "Done.\n",
      "---------------------------------------\n",
      "| eplenmean               | nan       |\n",
      "| eprewmean               | nan       |\n",
      "| fps                     | 952       |\n",
      "| loss/approxkl           | 8.89e-05  |\n",
      "| loss/clipfrac           | 0         |\n",
      "| loss/policy_entropy     | 5.54      |\n",
      "| loss/policy_loss        | -0.000819 |\n",
      "| loss/value_loss         | 0.00413   |\n",
      "| misc/explained_variance | 0.000305  |\n",
      "| misc/nupdates           | 4         |\n",
      "| misc/serial_timesteps   | 512       |\n",
      "| misc/time_elapsed       | 5.89      |\n",
      "| misc/total_timesteps    | 4.1e+03   |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stepping environment...\n",
      "Done.\n",
      "--------------------------------------\n",
      "| eplenmean               | nan      |\n",
      "| eprewmean               | nan      |\n",
      "| fps                     | 1.04e+03 |\n",
      "| loss/approxkl           | 0.00121  |\n",
      "| loss/clipfrac           | 0.0642   |\n",
      "| loss/policy_entropy     | 5.54     |\n",
      "| loss/policy_loss        | -0.00245 |\n",
      "| loss/value_loss         | 9.12e-07 |\n",
      "| misc/explained_variance | 0.128    |\n",
      "| misc/nupdates           | 5        |\n",
      "| misc/serial_timesteps   | 640      |\n",
      "| misc/time_elapsed       | 6.87     |\n",
      "| misc/total_timesteps    | 5.12e+03 |\n",
      "--------------------------------------\n",
      "Stepping environment...\n",
      "Done.\n",
      "--------------------------------------\n",
      "| eplenmean               | nan      |\n",
      "| eprewmean               | nan      |\n",
      "| fps                     | 1.01e+03 |\n",
      "| loss/approxkl           | 0.00069  |\n",
      "| loss/clipfrac           | 0.0198   |\n",
      "| loss/policy_entropy     | 5.54     |\n",
      "| loss/policy_loss        | -0.00186 |\n",
      "| loss/value_loss         | 0.00263  |\n",
      "| misc/explained_variance | 0.000395 |\n",
      "| misc/nupdates           | 6        |\n",
      "| misc/serial_timesteps   | 768      |\n",
      "| misc/time_elapsed       | 7.89     |\n",
      "| misc/total_timesteps    | 6.14e+03 |\n",
      "--------------------------------------\n",
      "Stepping environment...\n",
      "Done.\n",
      "---------------------------------------\n",
      "| eplenmean               | nan       |\n",
      "| eprewmean               | nan       |\n",
      "| fps                     | 907       |\n",
      "| loss/approxkl           | 8.09e-05  |\n",
      "| loss/clipfrac           | 0         |\n",
      "| loss/policy_entropy     | 5.54      |\n",
      "| loss/policy_loss        | -0.000117 |\n",
      "| loss/value_loss         | 1.95e-06  |\n",
      "| misc/explained_variance | 0.0474    |\n",
      "| misc/nupdates           | 7         |\n",
      "| misc/serial_timesteps   | 896       |\n",
      "| misc/time_elapsed       | 9.02      |\n",
      "| misc/total_timesteps    | 7.17e+03  |\n",
      "---------------------------------------\n",
      "Stepping environment...\n",
      "Done.\n",
      "--------------------------------------\n",
      "| eplenmean               | nan      |\n",
      "| eprewmean               | nan      |\n",
      "| fps                     | 920      |\n",
      "| loss/approxkl           | 0.000824 |\n",
      "| loss/clipfrac           | 0.0425   |\n",
      "| loss/policy_entropy     | 5.54     |\n",
      "| loss/policy_loss        | -0.0015  |\n",
      "| loss/value_loss         | 1.77e-06 |\n",
      "| misc/explained_variance | 0.0867   |\n",
      "| misc/nupdates           | 8        |\n",
      "| misc/serial_timesteps   | 1.02e+03 |\n",
      "| misc/time_elapsed       | 10.1     |\n",
      "| misc/total_timesteps    | 8.19e+03 |\n",
      "--------------------------------------\n",
      "Stepping environment...\n",
      "Done.\n",
      "---------------------------------------\n",
      "| eplenmean               | nan       |\n",
      "| eprewmean               | nan       |\n",
      "| fps                     | 958       |\n",
      "| loss/approxkl           | 0.0017    |\n",
      "| loss/clipfrac           | 0.118     |\n",
      "| loss/policy_entropy     | 5.54      |\n",
      "| loss/policy_loss        | -0.00195  |\n",
      "| loss/value_loss         | 1.14      |\n",
      "| misc/explained_variance | -0.000119 |\n",
      "| misc/nupdates           | 9         |\n",
      "| misc/serial_timesteps   | 1.15e+03  |\n",
      "| misc/time_elapsed       | 11.2      |\n",
      "| misc/total_timesteps    | 9.22e+03  |\n",
      "---------------------------------------\n",
      "Stepping environment...\n",
      "Done.\n",
      "---------------------------------------\n",
      "| eplenmean               | nan       |\n",
      "| eprewmean               | nan       |\n",
      "| fps                     | 886       |\n",
      "| loss/approxkl           | 2.17e-05  |\n",
      "| loss/clipfrac           | 0         |\n",
      "| loss/policy_entropy     | 5.54      |\n",
      "| loss/policy_loss        | -0.000352 |\n",
      "| loss/value_loss         | 0.000567  |\n",
      "| misc/explained_variance | 0.0177    |\n",
      "| misc/nupdates           | 10        |\n",
      "| misc/serial_timesteps   | 1.28e+03  |\n",
      "| misc/time_elapsed       | 12.4      |\n",
      "| misc/total_timesteps    | 1.02e+04  |\n",
      "---------------------------------------\n",
      "Stepping environment...\n",
      "Done.\n",
      "---------------------------------------\n",
      "| eplenmean               | nan       |\n",
      "| eprewmean               | nan       |\n",
      "| fps                     | 921       |\n",
      "| loss/approxkl           | 6.23e-05  |\n",
      "| loss/clipfrac           | 0         |\n",
      "| loss/policy_entropy     | 5.54      |\n",
      "| loss/policy_loss        | -0.000369 |\n",
      "| loss/value_loss         | 5.37e-05  |\n",
      "| misc/explained_variance | -0.00876  |\n",
      "| misc/nupdates           | 11        |\n",
      "| misc/serial_timesteps   | 1.41e+03  |\n",
      "| misc/time_elapsed       | 13.5      |\n",
      "| misc/total_timesteps    | 1.13e+04  |\n",
      "---------------------------------------\n",
      "Stepping environment...\n",
      "Done.\n",
      "--------------------------------------\n",
      "| eplenmean               | nan      |\n",
      "| eprewmean               | nan      |\n",
      "| fps                     | 946      |\n",
      "| loss/approxkl           | 0.000511 |\n",
      "| loss/clipfrac           | 0.00977  |\n",
      "| loss/policy_entropy     | 5.54     |\n",
      "| loss/policy_loss        | -0.00128 |\n",
      "| loss/value_loss         | 0.000128 |\n",
      "| misc/explained_variance | 0.000532 |\n",
      "| misc/nupdates           | 12       |\n",
      "| misc/serial_timesteps   | 1.54e+03 |\n",
      "| misc/time_elapsed       | 14.6     |\n",
      "| misc/total_timesteps    | 1.23e+04 |\n",
      "--------------------------------------\n",
      "Stepping environment...\n",
      "Done.\n",
      "---------------------------------------\n",
      "| eplenmean               | nan       |\n",
      "| eprewmean               | nan       |\n",
      "| fps                     | 844       |\n",
      "| loss/approxkl           | 0.000194  |\n",
      "| loss/clipfrac           | 0         |\n",
      "| loss/policy_entropy     | 5.54      |\n",
      "| loss/policy_loss        | -0.000552 |\n",
      "| loss/value_loss         | 3.78e-05  |\n",
      "| misc/explained_variance | 0.0182    |\n",
      "| misc/nupdates           | 13        |\n",
      "| misc/serial_timesteps   | 1.66e+03  |\n",
      "| misc/time_elapsed       | 15.8      |\n",
      "| misc/total_timesteps    | 1.33e+04  |\n",
      "---------------------------------------\n",
      "Stepping environment...\n",
      "Done.\n",
      "---------------------------------------\n",
      "| eplenmean               | nan       |\n",
      "| eprewmean               | nan       |\n",
      "| fps                     | 902       |\n",
      "| loss/approxkl           | 0.000381  |\n",
      "| loss/clipfrac           | 0.0083    |\n",
      "| loss/policy_entropy     | 5.53      |\n",
      "| loss/policy_loss        | -0.000669 |\n",
      "| loss/value_loss         | 7.33e-05  |\n",
      "| misc/explained_variance | -0.0155   |\n",
      "| misc/nupdates           | 14        |\n",
      "| misc/serial_timesteps   | 1.79e+03  |\n",
      "| misc/time_elapsed       | 16.9      |\n",
      "| misc/total_timesteps    | 1.43e+04  |\n",
      "---------------------------------------\n",
      "Stepping environment...\n",
      "Done.\n",
      "--------------------------------------\n",
      "| eplenmean               | nan      |\n",
      "| eprewmean               | nan      |\n",
      "| fps                     | 943      |\n",
      "| loss/approxkl           | 0.00133  |\n",
      "| loss/clipfrac           | 0.0786   |\n",
      "| loss/policy_entropy     | 5.52     |\n",
      "| loss/policy_loss        | -0.00227 |\n",
      "| loss/value_loss         | 2.79e-05 |\n",
      "| misc/explained_variance | -0.00315 |\n",
      "| misc/nupdates           | 15       |\n",
      "| misc/serial_timesteps   | 1.92e+03 |\n",
      "| misc/time_elapsed       | 18       |\n",
      "| misc/total_timesteps    | 1.54e+04 |\n",
      "--------------------------------------\n",
      "Stepping environment...\n",
      "Done.\n",
      "--------------------------------------\n",
      "| eplenmean               | nan      |\n",
      "| eprewmean               | nan      |\n",
      "| fps                     | 925      |\n",
      "| loss/approxkl           | 0.00122  |\n",
      "| loss/clipfrac           | 0.0693   |\n",
      "| loss/policy_entropy     | 5.53     |\n",
      "| loss/policy_loss        | 0.000297 |\n",
      "| loss/value_loss         | 1.34     |\n",
      "| misc/explained_variance | 8.18e-05 |\n",
      "| misc/nupdates           | 16       |\n",
      "| misc/serial_timesteps   | 2.05e+03 |\n",
      "| misc/time_elapsed       | 19.1     |\n",
      "| misc/total_timesteps    | 1.64e+04 |\n",
      "--------------------------------------\n",
      "Stepping environment...\n",
      "Done.\n",
      "---------------------------------------\n",
      "| eplenmean               | nan       |\n",
      "| eprewmean               | nan       |\n",
      "| fps                     | 957       |\n",
      "| loss/approxkl           | 8.59e-05  |\n",
      "| loss/clipfrac           | 0         |\n",
      "| loss/policy_entropy     | 5.53      |\n",
      "| loss/policy_loss        | -0.000336 |\n",
      "| loss/value_loss         | 0.00061   |\n",
      "| misc/explained_variance | -0.00475  |\n",
      "| misc/nupdates           | 17        |\n",
      "| misc/serial_timesteps   | 2.18e+03  |\n",
      "| misc/time_elapsed       | 20.2      |\n",
      "| misc/total_timesteps    | 1.74e+04  |\n",
      "---------------------------------------\n",
      "Stepping environment...\n",
      "Done.\n",
      "---------------------------------------\n",
      "| eplenmean               | nan       |\n",
      "| eprewmean               | nan       |\n",
      "| fps                     | 976       |\n",
      "| loss/approxkl           | 0.000192  |\n",
      "| loss/clipfrac           | 0         |\n",
      "| loss/policy_entropy     | 5.53      |\n",
      "| loss/policy_loss        | -0.000902 |\n",
      "| loss/value_loss         | 0.000131  |\n",
      "| misc/explained_variance | 0.00969   |\n",
      "| misc/nupdates           | 18        |\n",
      "| misc/serial_timesteps   | 2.3e+03   |\n",
      "| misc/time_elapsed       | 21.2      |\n",
      "| misc/total_timesteps    | 1.84e+04  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stepping environment...\n",
      "Done.\n",
      "---------------------------------------\n",
      "| eplenmean               | nan       |\n",
      "| eprewmean               | nan       |\n",
      "| fps                     | 985       |\n",
      "| loss/approxkl           | 5.12e-05  |\n",
      "| loss/clipfrac           | 0         |\n",
      "| loss/policy_entropy     | 5.53      |\n",
      "| loss/policy_loss        | -0.000118 |\n",
      "| loss/value_loss         | 9.69e-05  |\n",
      "| misc/explained_variance | -0.0256   |\n",
      "| misc/nupdates           | 19        |\n",
      "| misc/serial_timesteps   | 2.43e+03  |\n",
      "| misc/time_elapsed       | 22.3      |\n",
      "| misc/total_timesteps    | 1.95e+04  |\n",
      "---------------------------------------\n",
      "Stepping environment...\n",
      "Done.\n",
      "--------------------------------------\n",
      "| eplenmean               | nan      |\n",
      "| eprewmean               | nan      |\n",
      "| fps                     | 985      |\n",
      "| loss/approxkl           | 0.000205 |\n",
      "| loss/clipfrac           | 0        |\n",
      "| loss/policy_entropy     | 5.53     |\n",
      "| loss/policy_loss        | -0.00158 |\n",
      "| loss/value_loss         | 2.04e-05 |\n",
      "| misc/explained_variance | 0.0262   |\n",
      "| misc/nupdates           | 20       |\n",
      "| misc/serial_timesteps   | 2.56e+03 |\n",
      "| misc/time_elapsed       | 23.3     |\n",
      "| misc/total_timesteps    | 2.05e+04 |\n",
      "--------------------------------------\n",
      "Stepping environment...\n",
      "Done.\n",
      "--------------------------------------\n",
      "| eplenmean               | nan      |\n",
      "| eprewmean               | nan      |\n",
      "| fps                     | 1.07e+03 |\n",
      "| loss/approxkl           | 0.000567 |\n",
      "| loss/clipfrac           | 0.0083   |\n",
      "| loss/policy_entropy     | 5.52     |\n",
      "| loss/policy_loss        | 0.000324 |\n",
      "| loss/value_loss         | 0.357    |\n",
      "| misc/explained_variance | 0.00013  |\n",
      "| misc/nupdates           | 21       |\n",
      "| misc/serial_timesteps   | 2.69e+03 |\n",
      "| misc/time_elapsed       | 24.3     |\n",
      "| misc/total_timesteps    | 2.15e+04 |\n",
      "--------------------------------------\n",
      "Stepping environment...\n",
      "Done.\n",
      "---------------------------------------\n",
      "| eplenmean               | nan       |\n",
      "| eprewmean               | nan       |\n",
      "| fps                     | 910       |\n",
      "| loss/approxkl           | 0.000707  |\n",
      "| loss/clipfrac           | 0.0312    |\n",
      "| loss/policy_entropy     | 5.52      |\n",
      "| loss/policy_loss        | -0.00167  |\n",
      "| loss/value_loss         | 0.000112  |\n",
      "| misc/explained_variance | -0.000738 |\n",
      "| misc/nupdates           | 22        |\n",
      "| misc/serial_timesteps   | 2.82e+03  |\n",
      "| misc/time_elapsed       | 25.4      |\n",
      "| misc/total_timesteps    | 2.25e+04  |\n",
      "---------------------------------------\n",
      "Stepping environment...\n",
      "Done.\n",
      "---------------------------------------\n",
      "| eplenmean               | nan       |\n",
      "| eprewmean               | nan       |\n",
      "| fps                     | 926       |\n",
      "| loss/approxkl           | 0.00101   |\n",
      "| loss/clipfrac           | 0.033     |\n",
      "| loss/policy_entropy     | 5.52      |\n",
      "| loss/policy_loss        | -0.000682 |\n",
      "| loss/value_loss         | 0.000151  |\n",
      "| misc/explained_variance | 0.00895   |\n",
      "| misc/nupdates           | 23        |\n",
      "| misc/serial_timesteps   | 2.94e+03  |\n",
      "| misc/time_elapsed       | 26.5      |\n",
      "| misc/total_timesteps    | 2.36e+04  |\n",
      "---------------------------------------\n",
      "Stepping environment...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6394879e0844>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-6394879e0844>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0ment_coef\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2.5e-4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mcliprange\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     )\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/baselines/baselines/ppo2/ppo2.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(network, env, total_timesteps, eval_env, seed, nsteps, ent_coef, lr, vf_coef, max_grad_norm, gamma, lam, log_interval, nminibatches, noptepochs, cliprange, save_interval, load_path, model_fn, update_fn, init_fn, mpi_rank_weight, comm, **network_kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m# Get minibatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneglogpacs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepinfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#pylint: disable=E0632\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0meval_env\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0meval_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_returns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_neglogpacs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_epinfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_runner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#pylint: disable=E0632\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/baselines/baselines/ppo2/runner.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mmb_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmb_advs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmb_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         return (*map(sf01, (mb_obs, mb_returns, mb_dones, mb_actions, mb_values, mb_neglogpacs)),\n\u001b[0;32m---> 67\u001b[0;31m             mb_states, epinfos)\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;31m# obs, returns, masks, actions, values, neglogpacs, states = runner.run()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msf01\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/baselines/baselines/ppo2/runner.py\u001b[0m in \u001b[0;36msf01\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \"\"\"\n\u001b[1;32m     73\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train an agent using Proximal Policy Optimization from OpenAI Baselines\n",
    "\"\"\"\n",
    "import retro\n",
    "from baselines.common.vec_env import SubprocVecEnv\n",
    "from baselines.common.retro_wrappers import make_retro, wrap_deepmind_retro\n",
    "from baselines.ppo2 import ppo2\n",
    "game = 'Pitfall-Atari2600'\n",
    "state = retro.State.DEFAULT\n",
    "scenario = 'scenario'\n",
    "record = False\n",
    "verbose = 1\n",
    "quiet = 0\n",
    "obs_type = 'image'\n",
    "players = 1\n",
    "\n",
    "def main():\n",
    "    def make_env():\n",
    "        obs_type = retro.Observations.IMAGE # retro.Observations.RAM\n",
    "        env = retro.make(game=game, state=state, scenario=scenario, record=record, players=players, obs_type=obs_type)\n",
    "        # env = retro.make(game=game, state=state, scenario=scenario)\n",
    "        env = wrap_deepmind_retro(env)\n",
    "        return env\n",
    "    \n",
    "    venv = SubprocVecEnv([make_env] * 8)\n",
    "    ppo2.learn(\n",
    "        network='cnn', \n",
    "        env=venv, \n",
    "        total_timesteps=int(1e6),\n",
    "        nsteps=128,\n",
    "        nminibatches=4,\n",
    "        lam=0.95, \n",
    "        gamma=0.99, \n",
    "        noptepochs=4, \n",
    "        log_interval=1,\n",
    "        ent_coef=.01,\n",
    "        lr=lambda f : f * 2.5e-4,\n",
    "        cliprange=0.1,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "openai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
